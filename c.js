// questions-code.js 
// Questions relatives au code Python pour l'IA

const questionsCode = [
    {
        question: "Comment compléter ce code pour calculer le biais et la variance dans la décomposition de l'erreur de classification ?\ndef bias_variance_decomp_classification(model, X, y, n_splits=5):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    predictions = []\n    actual = []\n    \n    # Collecte des prédictions pour chaque fold\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n        ...",
        options: [
            "        model.fit(X_train_fold, y_train_fold)\n        # Prédictions sur l'ensemble de validation\n        pred_proba = model.predict_proba(X_val_fold)\n        predictions.append(pred_proba)\n        actual.append(y_val_fold)\n\n    # Convertir en arrays numpy pour faciliter les calculs\n    predictions = np.array(predictions)\n    actual = np.array(actual)\n\n    # Calcul du biais\n    avg_pred = np.mean(predictions, axis=0)\n    bias = np.mean((avg_pred.argmax(axis=1) - actual.flatten()) ** 2)\n\n    # Calcul de la variance\n    variance = np.mean(np.var(predictions, axis=0))\n\n    return bias, variance",
            "        model.fit(X_train_fold, y_train_fold)\n        # Prédictions sur l'ensemble de validation\n        pred = model.predict(X_val_fold)\n        predictions.append(pred)\n        actual.append(y_val_fold)\n\n    # Convertir en arrays numpy\n    predictions = np.array(predictions)\n    actual = np.array(actual)\n\n    # Calcul du biais\n    mean_pred = np.mean(predictions, axis=0)\n    bias = np.mean((mean_pred - actual.flatten()) ** 2)\n\n    # Calcul de la variance\n    var = np.zeros(len(predictions[0]))\n    for i in range(len(predictions)):\n        var += (predictions[i] - mean_pred) ** 2\n    variance = np.mean(var / len(predictions))\n\n    return bias, variance",
            "        model.fit(X_train_fold, y_train_fold)\n        # Prédictions sur l'ensemble de validation\n        pred = model.predict(X_val_fold)\n        predictions.append(pred)\n        actual.extend(y_val_fold)\n\n    # Convertir en arrays\n    predictions = np.array(predictions)\n    actual = np.array(actual)\n\n    # Calcul du biais et de la variance\n    ensemble_pred = np.mean(predictions, axis=0)\n    bias = np.mean((ensemble_pred - actual) ** 2)\n    variance = np.mean([np.mean((pred - ensemble_pred) ** 2) for pred in predictions])\n\n    return bias, variance",
            "        model.fit(X_train_fold, y_train_fold)\n        pred = model.predict_proba(X_val_fold)[:, 1]  # Prob. classe positive\n        predictions.append(pred)\n        actual.append(y_val_fold)\n\n    # Convertir en arrays numpy\n    predictions = np.stack(predictions, axis=0)\n    actual = np.concatenate(actual)\n\n    # Moyenne des prédictions de tous les modèles pour chaque instance\n    avg_pred = np.mean(predictions, axis=0)\n\n    # Biais - erreur quadratique entre prédiction moyenne et vérité\n    bias = np.mean((avg_pred - actual) ** 2)\n\n    # Variance - variance moyenne des prédictions pour chaque instance\n    variance = np.mean(np.var(predictions, axis=0))\n\n    return bias, variance"
        ],
        correctIndex: 3,
        explanation: "Cette solution est correcte car elle : (1) Utilise predict_proba pour obtenir les probabilités de la classe positive, essentiel pour la décomposition biais-variance en classification, (2) Empile correctement les prédictions avec np.stack pour garder la structure des folds, (3) Calcule le biais comme l'erreur quadratique moyenne entre la prédiction moyenne de tous les modèles et les valeurs réelles, (4) Calcule la variance comme la variance moyenne des prédictions entre les différents modèles pour chaque instance, capturant ainsi la variabilité due à la sensibilité aux données d'entraînement.",
        difficulty: "hard"
    },
    {
        question: "Comment implémenter la validation croisée pour un modèle XGBoost avec early stopping en Python ?",
        options: [
            "from sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\n# Création du modèle\nmodel = XGBClassifier(n_estimators=1000, early_stopping_rounds=10)\n\n# Validation croisée\nscores = cross_val_score(model, X, y, cv=5)",
            "from sklearn.model_selection import KFold\nimport xgboost as xgb\nimport numpy as np\n\ndef xgb_cv_with_early_stopping(X, y, params, num_folds=5, early_stopping_rounds=50):\n    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n    cv_scores = []\n    \n    for train_idx, val_idx in kf.split(X):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        # Créer les datasets XGBoost\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        \n        # Entraîner avec early stopping\n        model = xgb.train(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            evals=[(dtrain, 'train'), (dval, 'val')],\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=False\n        )\n        \n        # Évaluer le modèle sur l'ensemble de validation\n        pred = model.predict(dval)\n        score = compute_metric(y_val, pred)  # Fonction à définir selon la métrique souhaitée\n        cv_scores.append(score)\n    \n    return np.mean(cv_scores), np.std(cv_scores)",
            "from sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\n\ndef custom_cv_xgb(X, y, n_splits=5):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    scores = []\n    best_iterations = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        # Créer et entraîner le modèle avec early stopping\n        model = XGBClassifier(\n            n_estimators=1000,\n            learning_rate=0.1,\n            max_depth=6,\n            objective='binary:logistic'\n        )\n        \n        # Utiliser eval_set pour early stopping\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=20,\n            eval_metric='logloss',\n            verbose=False\n        )\n        \n        # Récupérer le nombre optimal d'itérations\n        best_iteration = model.best_iteration\n        best_iterations.append(best_iteration)\n        \n        # Scorer le modèle\n        score = model.score(X_val, y_val)\n        scores.append(score)\n        \n    # Retourner les résultats de la validation croisée\n    return np.mean(scores), np.std(scores), np.mean(best_iterations)",
            "from sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nimport numpy as np\n\ndef nested_cv_xgboost(X, y, param_grid, n_outer=5, n_inner=3):\n    # Boucle externe pour estimer la performance du modèle final\n    outer_cv = StratifiedKFold(n_splits=n_outer, shuffle=True, random_state=42)\n    outer_scores = []\n    \n    for outer_train_idx, test_idx in outer_cv.split(X, y):\n        X_outer_train, X_test = X[outer_train_idx], X[test_idx]\n        y_outer_train, y_test = y[outer_train_idx], y[test_idx]\n        \n        # Boucle interne pour la sélection d'hyperparamètres\n        inner_cv = StratifiedKFold(n_splits=n_inner, shuffle=True, random_state=43)\n        best_score = -np.inf\n        best_params = None\n        \n        for params in param_grid:\n            inner_scores = []\n            \n            for inner_train_idx, val_idx in inner_cv.split(X_outer_train, y_outer_train):\n                X_inner_train, X_val = X_outer_train[inner_train_idx], X_outer_train[val_idx]\n                y_inner_train, y_val = y_outer_train[inner_train_idx], y_outer_train[val_idx]\n                \n                # Convertir en DMatrix pour XGBoost\n                dtrain = xgb.DMatrix(X_inner_train, label=y_inner_train)\n                dval = xgb.DMatrix(X_val, label=y_val)\n                \n                # Entraîner avec early stopping\n                xgb_model = xgb.train(\n                    params,\n                    dtrain,\n                    num_boost_round=1000,\n                    evals=[(dval, 'val')],\n                    early_stopping_rounds=50,\n                    verbose_eval=False\n                )\n                \n                # Évaluer\n                val_pred = xgb_model.predict(dval)\n                score = calculate_metric(y_val, val_pred)  # Fonction à définir\n                inner_scores.append(score)\n            \n            # Moyenne des scores sur les plis internes\n            mean_score = np.mean(inner_scores)\n            if mean_score > best_score:\n                best_score = mean_score\n                best_params = params\n        \n        # Entraîner le modèle final avec les meilleurs paramètres\n        dtrain_full = xgb.DMatrix(X_outer_train, label=y_outer_train)\n        dtest = xgb.DMatrix(X_test, label=y_test)\n        \n        final_model = xgb.train(\n            best_params,\n            dtrain_full,\n            num_boost_round=1000,\n            evals=[(dtest, 'test')],\n            early_stopping_rounds=50,\n            verbose_eval=False\n        )\n        \n        # Évaluer le modèle final\n        test_pred = final_model.predict(dtest)\n        test_score = calculate_metric(y_test, test_pred)\n        outer_scores.append(test_score)\n    \n    # Retourner la performance moyenne sur les plis externes\n    return np.mean(outer_scores), np.std(outer_scores)"
        ],
        correctIndex: 2,
        explanation: "La méthode custom_cv_xgb est la plus appropriée pour implémenter la validation croisée avec early stopping pour XGBoost. En utilisant l'API scikit-learn de XGBoost (XGBClassifier), on peut facilement intégrer l'early stopping grâce au paramètre eval_set qui permet de spécifier un ensemble de validation, et early_stopping_rounds qui arrête l'entraînement si la métrique d'évaluation ne s'améliore pas pendant un certain nombre d'itérations. Cette approche est plus simple et directe que l'utilisation de l'API native de XGBoost (qui nécessite la création de DMatrix), tout en permettant de récupérer le nombre optimal d'itérations pour chaque fold.",
        difficulty: "hard"
    },
    {
        question: "Comment implémenter en Python une fonction pour évaluer l'importance des caractéristiques dans un modèle Random Forest basée sur la permutation ?",
        options: [
            "def permutation_importance(model, X, y, n_repeats=10):\n    # Score de base\n    baseline_score = model.score(X, y)\n    \n    # Calculer l'importance par permutation\n    n_features = X.shape[1]\n    importances = np.zeros(n_features)\n    importances_std = np.zeros(n_features)\n    \n    for i in range(n_features):\n        feature_importances = []\n        \n        for _ in range(n_repeats):\n            # Copier les données\n            X_permuted = X.copy()\n            \n            # Permuter la caractéristique i\n            X_permuted[:, i] = np.random.permutation(X_permuted[:, i])\n            \n            # Calculer la baisse de performance\n            permuted_score = model.score(X_permuted, y)\n            feature_importance = baseline_score - permuted_score\n            feature_importances.append(feature_importance)\n        \n        # Moyenne et écart-type sur les répétitions\n        importances[i] = np.mean(feature_importances)\n        importances_std[i] = np.std(feature_importances)\n    \n    return importances, importances_std",
            "def feature_importance(model, X):\n    # Récupérer l'importance des caractéristiques depuis le modèle\n    importances = model.feature_importances_\n    \n    # Calculer l'écart-type entre les arbres\n    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n    \n    # Créer un DataFrame pour les résultats\n    indices = np.argsort(importances)[::-1]\n    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n    \n    result = pd.DataFrame({\n        'feature': [feature_names[i] for i in indices],\n        'importance': importances[indices],\n        'std': std[indices]\n    })\n    \n    return result",
            "from sklearn.inspection import permutation_importance\n\ndef compute_feature_importance(model, X, y):\n    # Calculer l'importance des caractéristiques basée sur la permutation\n    result = permutation_importance(\n        model, X, y, n_repeats=10, random_state=42\n    )\n    \n    # Trier les caractéristiques par importance\n    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n    importances = pd.DataFrame({\n        'feature': feature_names,\n        'importance': result.importances_mean,\n        'std': result.importances_std\n    })\n    \n    return importances.sort_values('importance', ascending=False)",
            "def drop_column_importance(model, X, y):\n    # Score de base\n    baseline_score = model.score(X, y)\n    \n    # Calculer l'importance en supprimant chaque caractéristique\n    n_features = X.shape[1]\n    importances = np.zeros(n_features)\n    \n    for i in range(n_features):\n        # Créer un jeu de données sans la caractéristique i\n        X_dropped = np.delete(X, i, axis=1)\n        \n        # Ré-entraîner le modèle\n        model_dropped = clone(model)\n        model_dropped.fit(X_dropped, y)\n        \n        # Calculer la baisse de performance\n        dropped_score = model_dropped.score(X_dropped, y)\n        importances[i] = baseline_score - dropped_score\n    \n    return importances"
        ],
        correctIndex: 0,
        explanation: "La première option implémente correctement l'importance par permutation, qui est une méthode model-agnostic pour évaluer l'importance des caractéristiques. L'idée fondamentale est de mesurer la baisse de performance lorsqu'une caractéristique spécifique est 'cassée' (rendue aléatoire par permutation), tout en laissant les autres intactes. Si la performance chute significativement, cela signifie que la caractéristique est importante. Contrairement à l'attribut feature_importances_ natif de RandomForest (qui est basé sur la diminution de l'impureté), la méthode par permutation peut capturer des interactions complexes et fonctionne même après l'entraînement du modèle. La répétition multiple (n_repeats) permet d'obtenir une estimation plus stable et de quantifier l'incertitude via l'écart-type.",
        difficulty: "hard"
    },
    {
        question: "Parmi les implémentations suivantes pour la normalisation des caractéristiques par lot (batch normalization), laquelle est correcte ?",
        options: [
            "def batch_normalization(X, epsilon=1e-8):\n    # Normaliser chaque caractéristique\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0)\n    return (X - mean) / (std + epsilon)",
            "import tensorflow as tf\n\ndef add_batch_norm(model, inputs):\n    return tf.keras.layers.BatchNormalization()(inputs)",
            "class BatchNormalization:\n    def __init__(self, momentum=0.9, epsilon=1e-5):\n        self.momentum = momentum\n        self.epsilon = epsilon\n        self.running_mean = None\n        self.running_var = None\n        self.gamma = None  # Scale parameter\n        self.beta = None   # Shift parameter\n        \n    def forward(self, X, training=True):\n        # Initialize parameters if not done yet\n        if self.gamma is None:\n            self.gamma = np.ones(X.shape[1])\n            self.beta = np.zeros(X.shape[1])\n            self.running_mean = np.zeros(X.shape[1])\n            self.running_var = np.ones(X.shape[1])\n        \n        if training:\n            # Compute batch statistics\n            batch_mean = np.mean(X, axis=0)\n            batch_var = np.var(X, axis=0)\n            \n            # Update running statistics\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n            \n            # Normalize\n            X_norm = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n        else:\n            # Use running statistics for inference\n            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n        \n        # Scale and shift\n        return self.gamma * X_norm + self.beta",
            "from sklearn.preprocessing import StandardScaler\n\ndef normalize_features(X_train, X_test):\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled"
        ],
        correctIndex: 2,
        explanation: "La troisième option implémente correctement la batch normalization telle que décrite dans l'article original de Ioffe et Szegedy (2015). Les éléments clés sont : (1) Calcul des statistiques par lot pendant l'entraînement (moyenne et variance de chaque mini-lot), (2) Maintien de statistiques glissantes (running_mean, running_var) pour l'inférence, contrôlées par le paramètre momentum, (3) Normalisation avec un terme epsilon pour éviter la division par zéro, (4) Paramètres apprenables gamma (échelle) et beta (décalage) qui permettent au réseau d'annuler la normalisation si nécessaire. Les autres options manquent certains aspects essentiels : la première est une simple normalisation Z-score, la seconde est juste un appel à l'API TensorFlow sans détails d'implémentation, et la quatrième est une standardisation globale avec scikit-learn, pas une normalisation par lot.",
        difficulty: "hard"
    }
];

// Exporter les questions
if (typeof module !== 'undefined') {
    module.exports = { questionsCode };
}